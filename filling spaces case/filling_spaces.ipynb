{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14d062cd",
   "metadata": {},
   "source": [
    "Цель — обучить модель, которая принимает на вход строку без пробелов и восстанавливает корректное расположение пробелов. Для решения пользуемся ByT5-small, работающая на уровне байтов, гуд для обработки текстов без токенизации по словам. В обучение я взял данные Wikipedia RU (Hugging Face datasets) из исходных удалял пробелы отправлял на вход, оригинальный текст использовал как целевой выход, файнтюним ByT5-small на CPU 5к тренировочных пар одна эпоха. На инференсе генерируется текст с пробелами, а потом преобразуется в список индексов пробелов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f0fb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import random\n",
    "import torch\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79405730",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"google/byt5-small\"\n",
    "OUTPUT_DIR = \"./byt5-spaces\"\n",
    "MAX_LEN = 128\n",
    "TRAIN_SAMPLES = 5000\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 16       \n",
    "LR = 5e-4\n",
    "LOG_STEPS = 50\n",
    "\n",
    "TEST_CSV = \"dataset_1937770_3.txt\"   #мой тестовый датасет \n",
    "SUBMISSION_CSV = \"submission.csv\"\n",
    "SEED = 42\n",
    "\n",
    "#несколько утилит \n",
    "def set_seed(seed=42): #воспроизводимость \n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "def normalize_whitespace(text: str) -> str: #удаление лишних пробелов\n",
    "    return re.sub(r\"\\s+\", \" \", str(text)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a361a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pairs_from_texts(texts, max_samples=20000, max_len=128): #строим пары (вход без пробелов, цель с пробелами)\n",
    "    pairs = []\n",
    "    for t in texts:\n",
    "        if not t or not isinstance(t, str):\n",
    "            continue\n",
    "        t = normalize_whitespace(t)\n",
    "        if len(t) < 10 or \" \" not in t:\n",
    "            continue\n",
    "        step = max_len // 2 if max_len >= 8 else max_len\n",
    "        for i in range(0, len(t), step):\n",
    "            chunk = t[i:i + max_len]\n",
    "            if len(chunk) < 5 or \" \" not in chunk:\n",
    "                continue\n",
    "            # Удаляем ВСЕ типы пробелов\n",
    "            inp = re.sub(r\"\\s+\", \"\", chunk)\n",
    "            if len(inp) < 3:\n",
    "                continue\n",
    "            pairs.append({\"input\": inp, \"target\": chunk})\n",
    "            if len(pairs) >= max_samples:\n",
    "                return pairs\n",
    "    return pairs\n",
    "\n",
    "def preprocess_function(examples, tokenizer, max_len=128): #токенизация входа и таргета\n",
    "    #токенизация входа\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"input\"], \n",
    "        max_length=max_len, \n",
    "        truncation=True, \n",
    "        padding=False\n",
    "    )\n",
    "    #токенизация таргета\n",
    "    labels = tokenizer(\n",
    "        examples[\"target\"], \n",
    "        max_length=max_len, \n",
    "        truncation=True, \n",
    "        padding=False\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "def restore_spaces_batch(texts, model, tokenizer, max_len=128): #\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = tokenizer(\n",
    "        texts, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=max_len\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs, \n",
    "            max_length=max_len, \n",
    "            num_beams=4   # улучшает результаты\n",
    "        )\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True) #возвращаем список текстов с пробелами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81112e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cид и принуждаем к CPU \n",
    "set_seed(SEED)\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbdf3f6",
   "metadata": {},
   "source": [
    "Данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a75695",
   "metadata": {},
   "outputs": [],
   "source": [
    "#загрузка википедии\n",
    "wiki = load_dataset(\"wikimedia/wikipedia\", \"20231101.ru\", split=\"train[:200000]\") #загружаем википедию \n",
    "\n",
    "#формируем пары\n",
    "pairs = build_pairs_from_texts( #формируем тренировочные пары \n",
    "    wiki[\"text\"], \n",
    "    max_samples=TRAIN_SAMPLES, \n",
    "    max_len=MAX_LEN\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(pairs)\n",
    "train_ds = Dataset.from_pandas(df)\n",
    "\n",
    "#токенизация\n",
    "tokenized_train = train_ds.map(\n",
    "    lambda ex: preprocess_function(ex, tokenizer, MAX_LEN),\n",
    "    batched=True, \n",
    "    remove_columns=train_ds.column_names\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8f5444",
   "metadata": {},
   "source": [
    "Трейним (около 12 минут)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177a24a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=LOG_STEPS,\n",
    "    dataloader_num_workers=0, #иначе ложится \n",
    "    group_by_length=False,\n",
    "    optim=\"adafactor\",\n",
    "    eval_strategy =\"no\",   \n",
    "    fp16=False,                 \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "#обучение\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6001e08",
   "metadata": {},
   "source": [
    "Инферинсим (тоже около 10 минут)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd6e240",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TEST_CSV' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Инференс\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mЗагрузка тестового файла:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mTEST_CSV\u001b[49m)\n\u001b[1;32m      5\u001b[0m rows \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(TEST_CSV, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f: \u001b[38;5;66;03m#там есть гадкие строки, где запятые внутри текста \u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TEST_CSV' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\nЗагрузка тестового файла:\", TEST_CSV)\n",
    "rows = []\n",
    "with open(TEST_CSV, encoding=\"utf-8\") as f: #там есть гадкие строки, где запятые внутри текста \n",
    "    header = f.readline().strip().split(\",\")  # ['id', 'text_no_spaces']\n",
    "    for line in f:\n",
    "        line = line.rstrip(\"\\n\")\n",
    "        if not line:\n",
    "            continue\n",
    "        id_part, text_part = line.split(\",\", 1)  #делим только по первой запятой\n",
    "        rows.append((id_part, text_part))\n",
    "\n",
    "import pandas as pd\n",
    "test_df = pd.DataFrame(rows, columns=header)\n",
    "assert \"text_no_spaces\" in test_df.columns, \"В тестовом CSV должен быть столбец 'text_no_spaces'.\"\n",
    "\n",
    "texts = test_df[\"text_no_spaces\"].astype(str).tolist()\n",
    "\n",
    "print(\"Генерация результатов\")\n",
    "BATCH_GEN = 32\n",
    "restored_all = []\n",
    "for i in range(0, len(texts), BATCH_GEN):\n",
    "    batch = texts[i:i+BATCH_GEN]\n",
    "    restored_all.extend(restore_spaces_batch(batch, model, tokenizer, MAX_LEN))\n",
    "\n",
    "test_df[\"restored_text\"] = restored_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128261a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сохранено: submission.csv\n",
      "  id  predicted_positions\n",
      "0  0          [5, 10, 12]\n",
      "1  1            [3, 6, 7]\n",
      "2  2  [4, 12, 13, 20, 21]\n",
      "3  3          [5, 10, 18]\n",
      "4  4           [2, 5, 10]\n",
      "5  5           [6, 7, 13]\n",
      "6  6              [5, 14]\n",
      "7  7          [3, 12, 15]\n",
      "8  8              [6, 13]\n",
      "9  9                  [7]\n"
     ]
    }
   ],
   "source": [
    "#получаем позиции пробелов\n",
    "def get_space_positions(orig_text, restored_text):\n",
    "    positions = []\n",
    "    i, j = 0, 0\n",
    "    while i < len(orig_text) and j < len(restored_text):\n",
    "        if restored_text[j] == \" \":\n",
    "            # пробел в restored перед символом orig[i]\n",
    "            positions.append(i)  \n",
    "            j += 1\n",
    "        else:\n",
    "            i += 1\n",
    "            j += 1\n",
    "    return positions\n",
    "\n",
    "test_df[\"predicted_positions\"] = [\n",
    "    str(get_space_positions(orig, restored))\n",
    "    for orig, restored in zip(test_df[\"text_no_spaces\"], test_df[\"restored_text\"])\n",
    "]\n",
    "\n",
    "#сохраняем финальный submission\n",
    "submission = test_df[[\"id\", \"predicted_positions\"]]\n",
    "submission.to_csv(SUBMISSION_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"Сохранено:\", SUBMISSION_CSV)\n",
    "print(submission.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
